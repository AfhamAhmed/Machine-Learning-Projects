nb_pred <- predict(nb1, test[,-10])
t<-table(nb_pred, test$mpglevel)
t
print(paste("Mean Accuracy = ", mean(nb_pred == test$mpglevel)))
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="linear", cost=0.01, scale=FALSE)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="polynomial", cost=100, degree=3)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="polynomial", cost=100, degree=3)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="radial", cost=1, gamma = 1)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1234)
tune.out = tune(svm, mpglevel ~ ., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree =c(0.5,1,2,3,4)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
svm1 <- svm(mpglevel ~ ., data = train, kernel = "polynomial",cost = 100, degree = 3)
#predict
pred2 <- predict(svm1, test[,-8])
table(pred2, test[,8])
mean(pred2 == test[,8])
set.seed(1)
tune.out = tune(svm, mpglevel ~ ., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree =c(0.5,1,2,3,4)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
svm1 <- svm(mpglevel ~ ., data = train, kernel = "polynomial",cost = 100, degree = 3)
#predict
pred2 <- predict(svm1, test[,-8])
table(pred2, test[,8])
mean(pred2 == test[,8])
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1234)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
naiveBayes1 <- naiveBayes(mpglevel~., data=train)
naiveBayesPred <- predict(naiveBayes1, test[,-10])
table <- table(naiveBayesPred, test$mpglevel)
table
print(paste("Mean Accuracy is = ", mean(naiveBayesPred == test$mpglevel)))
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
naiveBayes1 <- naiveBayes(mpglevel~., data=train)
naiveBayesPred <- predict(naiveBayes1, test[,-10])
table <- table(naiveBayesPred, test$mpglevel)
table
print(paste("Mean Accuracy is = ", mean(naiveBayesPred == test$mpglevel)))
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="linear", cost=0.01, scale=FALSE)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="polynomial", cost=100, degree=3)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="polynomial", cost=10, degree=3)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="radial", cost=1, gamma = 1)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
set.seed(1)
tune.out = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
svmfit = svm(mpglevel~., data=train, kernel="radial", cost=1, gamma = 0.5)
summary(svmfit)
pred.te = predict(svmfit, newdata=test)
pred.te
mean(pred.te==test$mpglevel)
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1234)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1234)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
naiveBayes1 <- naiveBayes(mpglevel~., data=train)
naiveBayesPred <- predict(naiveBayes1, test[,-10])
table <- table(naiveBayesPred, test$mpglevel)
table
print(paste("Mean Accuracy is = ", mean(naiveBayesPred == test$mpglevel)))
set.seed(1)
tuneOut = tune(svm, mpglevel~., data=train, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuneOut)
svmFit = svm(mpglevel~., data=train, kernel="linear", cost=0.01, scale=FALSE)
summary(svmFit)
predictData = predict(svmFit, newdata=test)
predictData
mean(predictData == test$mpglevel)
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1234)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
#Naive Bayes Model.
naiveBayes1 <- naiveBayes(mpglevel~., data=train)
#Predicting Naive Bayes
naiveBayesPred <- predict(naiveBayes1, test[,-10])
#Making a table to check actual values versus predicted.
table <- table(naiveBayesPred, test$mpglevel)
#Printing out the table.
table
print(paste("The Mean Accuracy for Naive Bayes is = ", mean(naiveBayesPred == test$mpglevel)))
#setting the seed to 1
set.seed(1234)
#Tuning to check the best value of cost.
tuneOut = tune(svm, mpglevel~., data=train, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tuneOut)
#Making an SVM Model that with kernel set to linear.
svmFit = svm(mpglevel~., data = train, kernel = "linear", cost = 0.01, scale = FALSE)
summary(svmFit)
#Predicting Data on Test.
predictData = predict(svmFit, newdata=test)
predictData
#Calculating mean and printing it out.
mean(predictData == test$mpglevel)
#Setting seed to 1.
set.seed(1234)
#Tuning to check the best cost and degree.
tuneOut = tune(svm, mpglevel~., data = train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree=c(0.5,1,2,3,4)))
#Summary function on tuneOut.
summary(tuneOut)
#SVM Model with kernel polynomial.
svmFit = svm(mpglevel~., data=train, kernel = "polynomial", cost = 10, degree = 3)
summary(svmFit)
#Predicting data
predictData = predict(svmFit, newdata=test)
predictData
#Calculating mean accuracy.
mean(predictData == test$mpglevel)
#Setting seed to 1
set.seed(1234)
#Tuning to find the best cost and gamma.
tuneOut = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
#Summary on tune.
summary(tuneOut)
#SVM Model with kernel Radial and best values for cost and gamma.
svmFit = svm(mpglevel~., data = train, kernel = "radial", cost = 1, gamma = 1)
summary(svmFit)
#Predicting data.
predictData = predict(svmfit, newdata=test)
#Setting seed to 1
set.seed(1234)
#Tuning to find the best cost and gamma.
tuneOut = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
#Summary on tune.
summary(tuneOut)
#SVM Model with kernel Radial and best values for cost and gamma.
svmFit = svm(mpglevel~., data = train, kernel = "radial", cost = 1, gamma = 1)
summary(svmFit)
#Predicting data.
predictData = predict(svmFit, newdata=test)
predictData
#Finding mean accuracy.
mean(predictData == test$mpglevel)
#Setting seed to 1.
set.seed(1234)
#Tuning to check the best cost and degree.
tuneOut = tune(svm, mpglevel~., data = train, kernel="polynomial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), degree=c(0.5,1,2,3,4)))
#Summary function on tuneOut.
summary(tuneOut)
#SVM Model with kernel polynomial.
svmFit = svm(mpglevel~., data=train, kernel = "polynomial", cost = 100, degree = 3)
summary(svmFit)
#Predicting data
predictData = predict(svmFit, newdata=test)
predictData
#Calculating mean accuracy.
mean(predictData == test$mpglevel)
summary(naiveBayes1)
#Setting seed to 1234.
set.seed(1234)
#Tuning to find the best cost and gamma.
tuneOut = tune(svm, mpglevel~., data=train, kernel="radial",
ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), gamma=c(0.5,1,2,3,4)))
#Summary on tune.
summary(tuneOut)
#SVM Model with kernel Radial and best values for cost and gamma.
svmFit = svm(mpglevel~., data = train, kernel = "radial", cost = 1, gamma = 1)
summary(svmFit)
#Predicting data.
predictData = predict(svmFit, newdata=test)
predictData
#Finding mean accuracy.
mean(predictData == test$mpglevel)
#load library ISLR
library(ISLR)
#Loding library e1071.
library(e1071)
#get median value using median function.
medianValue = median(Auto$mpg)
#create a var mpgLevel that is 1 if mpg > medianValue, otherwise it is zero.
mpgLevel = ifelse(Auto$mpg > medianValue, 1, 0)
#Add it to the dataset Auto.
Auto$mpglevel = as.factor(mpgLevel)
#Checking if dataset is correct and mpglevel is added.
names(Auto)
#---------------------  DIVIDING DATA INTO TEST AND TRAIN -------------------------------
#setting seed to 1234 so results are same for all.
set.seed(1234)
ind <- sample(2, nrow(Auto), replace=TRUE, prob=c(0.75, 0.25))
#-c(1,9) is there so that mpg and name are excluded as given in the instructions.
train <- Auto[ind==1,-c(1,9)]
test <- Auto[ind==2,-c(1,9)]
#setting up a 2x2 graph.
par(mfrow=c(2,2))
#plotting a 2x2 with horsepower~mpg and weight~mpg
plot(Auto$horsepower~Auto$mpg, col=(Auto$mpglevel))
plot(Auto$weight~Auto$mpg, col=(Auto$mpglevel))
#plotting horsepower~mpglevel and weight~mpglevel
plot(Auto$horsepower~Auto$mpglevel)
plot(Auto$weight~Auto$mpglevel)
setwd("C:/Users/Faraz Khalid/Desktop/Faraz Khalid/University/Spring 2018/Machine Learning/Nueral Network Project")
dataframe <- read.csv('bank_auth.csv')
#Check if the data frame is loaded correctly or not.
head(dataframe)
dataframe <- read.csv('bank_auth_data.csv')
#Check if the data frame is loaded correctly or not.
head(dataframe)
dataframe <- read.csv('bank_auth_data.csv')
#Check if the data frame is loaded correctly or not.
head(dataframe)
tail(dataframe)
str(dataframe)
dataframe <- read.csv('bank_auth_data.csv')
#Check if the data frame is loaded correctly or not.
head(dataframe)
tail(dataframe)
str(dataframe)
library(caTools)
set.seed(2018)
split <- sample.split(dataframe$Class, SplitRatio = 0.7)
trainSet <- subset(dataframe, split == TRUE)
testSet <- subset(dataframe, split == FALSE)
library(caTools)
set.seed(2018)
split <- sample.split(dataframe$Class, SplitRatio = 0.7)
trainSet <- subset(dataframe, split == TRUE)
testSet <- subset(dataframe, split == FALSE)
library(neuralnet)
install.packages('neuralnet')
library(neuralnet)
help("neuralnet")
NueralNetworkModel <- nueralnet(Class ~ ., data = trainSet, hidden = c(5,3), linear.output = FALSE)
NueralNetworkModel <- neuralnet(Class ~ ., data = trainSet, hidden = c(5,3), linear.output = FALSE)
NueralNetworkModel <- neuralnet(Class ~ Image.Var+Image.Skew+Image.Curt+Entropy,
data = trainSet, hidden = c(5,3), linear.output = FALSE)
predictions <- compute(NueralNetworkModel, test[1:4])
NueralNetworkModel <- neuralnet(Class ~ Image.Var+Image.Skew+Image.Curt+Entropy,
data = trainSet, hidden = c(5,3), linear.output = FALSE)
predictions <- compute(NueralNetworkModel, testSet[1:4])
head(predictions$net.result)
NueralNetworkModel <- neuralnet(Class ~ Image.Var+Image.Skew+Image.Curt+Entropy,
data = trainSet, hidden = c(5,3), linear.output = FALSE)
predictions <- compute(NueralNetworkModel, testSet[1:4])
predictions <- sapply(predictions$net.result, round)
head(predictions$net.result)
NueralNetworkModel <- neuralnet(Class ~ Image.Var+Image.Skew+Image.Curt+Entropy,
data = trainSet, hidden = c(5,3), linear.output = FALSE)
predictions <- compute(NueralNetworkModel, testSet[1:4])
predictions <- sapply(predictions$net.result, round)
head(predictions)
#Table to create confusion matrix
table(predictions, testSet$Class)
install.packages(randomForest)
dataframe$Class = factor(df$Class)
install.packages(randomForest)
library(randomForest)
install.packages('randomForest')
library(randomForest)
dataframe$Class = factor(dataframe$Class)
library(caTools)
set.seed(2018)
split <- sample.split(dataframe$Class, SplitRatio = 0.7)
trainSet <- subset(dataframe, split == TRUE)
testSet <- subset(dataframe, split == FALSE)
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
paste((226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
paste((226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
paste("Accuracy for Random Forest " (226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
print("Hello")
paste((226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
accuracy((226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
accuracy((226+183)/(226+183+3))
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
accuracy = ((226+183)/(226+183+3))
print("Hello" + accuracy)
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
accuracy = ((226+183)/(226+183+3))
print("Hello", accuracy)
install.packages("GGally")
library(GGally)
library(network)
install.packages("GGally")
library(GGally)
install.packages("network")
library(network)
install.packages("sna")
install.packages("GGally")
ggnet2(dataframe)
ggnet2(dataframe)
net = rgraph(10, mode = "graph", tprob = 0.5)
dataframe <- read.csv('bank_auth_data.csv')
#Check if the data frame is loaded correctly or not.
head(dataframe)
tail(dataframe)
str(dataframe)
library(caTools)
set.seed(2018)
split <- sample.split(dataframe$Class, SplitRatio = 0.7)
trainSet <- subset(dataframe, split == TRUE)
testSet <- subset(dataframe, split == FALSE)
install.packages('neuralnet')
library(neuralnet)
#If you do not know what Neural Net is, help function would be a great way to discover it.
help("neuralnet")
NueralNetworkModel <- neuralnet(Class ~ Image.Var+Image.Skew+Image.Curt+Entropy,
data = trainSet, hidden = c(5,3), linear.output = FALSE)
predictions <- compute(NueralNetworkModel, testSet[1:4])
predictions <- sapply(predictions$net.result, round)
head(predictions)
#Table to create confusion matrix
table(predictions, testSet$Class)
install.packages('randomForest')
library(randomForest)
dataframe$Class = factor(dataframe$Class)
library(caTools)
set.seed(2018)
split <- sample.split(dataframe$Class, SplitRatio = 0.7)
trainSet <- subset(dataframe, split == TRUE)
testSet <- subset(dataframe, split == FALSE)
randomForestModel <- randomForest(Class ~ ., data = trainSet)
randomForestPred <- predict(randomForestModel, testSet)
table(randomForestPred, testSet$Class)
accuracy = ((226+183)/(226+183+3))
print(accuracy)
install.packages("neuralnet")
